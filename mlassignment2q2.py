# -*- coding: utf-8 -*-
"""MLAssignment2Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m4JZX83xgYOvd5zWrFQMsAgn9tFRRlfw

**Answer 2b**
"""

class LogRegression(object):
    """docstring for LogRegression."""
    def __init__(self, X, learning_rate=0.1, num_iters=1000):
        super(LogRegression, self).__init__()
        self.learning_rate = learning_rate
        self.num_iters = num_iters
        # m for #training examples, n for #features
        self.m , self.n = X.shape
        self.theta = None
        self.intercept = None

    def sigmoid(self,x):
        return 1 / (1 + np.exp(-x))

    """You can give any required inputs to the fit()"""
    def fit(self, X, y, reg, reg_constant, class_of_interest, multiclass):
        self.theta = np.zeros((self.n,1)) # column vector 
        self.intercept = 0
        self.mclass = 0
        loss_reglst =[]
        accuracy_iterlst = []
        y = y[:, np.newaxis]        
        y_binary = np.zeros((y.shape))
        y_binary = np.where(y == class_of_interest, 1, y_binary)
        
        for i in range(self.num_iters+1):

            # calculate hypothesis
            y_probab = self.sigmoid((X @ self.theta)+self.intercept) #w-transpose.X+w0
            
            #calculate cost
            error = 1e-5
            if multiclass == 0 :
                if (reg == 0):
                    #loss_reg = -1/self.m * np.sum(y*np.log(y_probab)+(1-y)*np.log(1-y_probab))
                    loss_reg = -(1/self.m)*np.sum(((y)* np.log(y_probab + error))+((1-y)*np.log(1-y_probab + error)))
                elif (reg == 1):
                    loss_reg = -(1/self.m)*np.sum(((y)* np.log(y_probab + error))+((1-y)*np.log(1-y_probab + error))) + reg_constant/(2*self.m)*np.sum(self.theta**2)
            elif multiclass == 1 and reg == 1:
                
                loss_reg = -1/self.m * np.sum(y_binary*np.log(y_probab + error)+(1-y_binary)*np.log(1-y_probab + error)) + reg_constant/(2*self.m)*np.sum(self.theta**2)
            
            #calculate accuracy            
            pred_labels = y_probab > 0.5
            # neg_pred_labels = y_probab < 0.5
            correct_class = 0
            total_class = 0
            if multiclass == 0 :
                accuracy_iter = np.sum(y == pred_labels)/X.shape[0]
            elif multiclass == 1:
                
                accuracy_iter = np.sum(y_binary == pred_labels)/X.shape[0] 
            
            # gradient descent
            if multiclass == 0 :
                if (reg == 0):
                  dtheta = 1/self.m * np.dot(X.T, (y_probab-y)) 
                elif (reg == 1):
                  dtheta = 1/self.m * np.dot(X.T, (y_probab-y)) + (reg_constant/self.m)*self.theta
                dintercept = 1/self.m * np.sum(y_probab-y)

            elif multiclass == 1 and reg == 1:
                # self.mclass = 1
                # coi = [0, 1, 2, 3]
                # list1 = []
                # list2 = []
                # for i in coi :

                #     y_binary = np.where(y == coi, 1, y_binary)
                #     lossreg = -1/self.m * np.sum(y_binary*np.log(y_probab)+(1-y_binary)*np.log(1-y_probab)) + + reg_constant/(2*self.m)*np.sum(self.theta**2)
                
                #     accuracy_iter = np.sum(y_binary == pred_labels)/X.shape[0]
                #     list1.append(predict_labels)
                #     list2.append(accuracy_iter)
                #     dtheta = 1/self.m * np.dot(X.T, (y_probab-y_binary)) + (reg_constant/self.m)*self.theta
                #     dintercept = 1/self.m * np.sum(y_probab-y_binary)
                #     self.theta -= self.learning_rate * dtheta
                #     self.intercept -= self.learning_rate * dintercept
                      # list3.append(theta)
                      # list4.append(intercept)


                dtheta = 1/self.m * np.dot(X.T, (y_probab-y_binary)) + (reg_constant/self.m)*self.theta
                dintercept = 1/self.m * np.sum(y_probab-y_binary)

            self.theta -= self.learning_rate * dtheta
            self.intercept -= self.learning_rate * dintercept
            
            if i % 200 == 0:
                loss_reglst.append(loss_reg)
                accuracy_iterlst.append(accuracy_iter)
                #print(f'Cost after iteration {i} : {cost}')
        
        return self.theta, self.intercept, loss_reglst, accuracy_iterlst
      
        """Write it from scratch. Usage of sklearn is not allowed"""

    """ You can add as many methods according to your requirements, but training must be using fit(), and testing must be with predict()"""

    def probab_predict(self, X_test):
        y_predict = self.sigmoid(np.dot(X_test, self.theta) + self.intercept)
        return (y_predict).flatten()

    def predict(self, X_test):

        y_predict = self.sigmoid(np.dot(X_test, self.theta) + self.intercept)
        y_predicted_class = [ 1 if i>0.5 else 0 for i in y_predict]
        # y_predict_labels = y_predict > 0.5
        # if y_predict > 0.5:
        #   y_predict_labels = 1
        # else :
        #   y_predict_labels = 0

        #print(type(y_predict_labels))
        #return y_predict_labels
        # if multiclass == 1:
           # y_predict = self.sigmoid(np.dot(X_test, self.theta) + self.intercept) theta1...theta4
           #then calculate y_predict
        return y_predicted_class
        #return np.round(y_predict).flatten()

        """Write it from scratch. Usage of sklearn is not allowed"""

        """Fill your code here. predict() should only take X_test and return predictions."""

"""**2 Part a**"""

def q2a():
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.io import loadmat
  import seaborn as sns
  mats = loadmat('/content/drive/My Drive/Colab Notebooks/MLAssignment2/dataset_1.mat')

  data=mats['samples']
  labels=mats['labels'][0]
  unique_labels=np.unique(labels)
  x=np.zeros(5000)
  y=np.zeros(5000)
  for i in range(len(labels)):
    x[i]=data[i][0]
    y[i]=data[i][1]
  plt.figure(figsize=(12,6))
  scatter=plt.scatter(x,y,c=mats["labels"],cmap=plt.cm.get_cmap("prism",2),marker='o',s=15)
  #plt.colorbar(ticks=range(2))
  plt.xlabel("feature1")
  plt.ylabel("feature2")
  plt.legend(handles=scatter.legend_elements()[0],labels=list(unique_labels))
  plt.title("scatter plot dor dataset1.mat")
  plt.show()

  # sns.set_style('white')
  # #labels_new= labels[:,np.newaxis]
  # sns.scatterplot(x,y,hue=labels)

q2a()



"""**2 Part c**"""

def q2c():
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.datasets import make_classification
  import seaborn as sns
  from scipy.io import loadmat
  from random import seed,randrange
  import pandas as pd
  import joblib

  mats = loadmat('/content/drive/My Drive/Colab Notebooks/MLAssignment2/dataset_1.mat')

  data=mats['samples']
  labels=mats['labels'][0]
  unique_labels=np.unique(labels)

  # def k_split(dataset,labels,k):
  #   data_folds = np.array_split(dataset,k)
  #   label_folds = np.array_split(labels,k)
  #   return data_folds, label_folds
  # datafolds,labelfolds = k_split(data, labels,5)

  np.random.seed(1)
  # Split a dataset into k folds
  def kfold_split(dataset,labels,folds):
    dataset_split=list()
    labels_split=list()
    dataset_copy=dataset.tolist()
    labels_copy=labels.tolist()
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
      fold=list()
      foldlabel=list()
      while len(fold)<fold_size:
        index = randrange(len(dataset_copy))
        #p=np.random.permutation(len(dataset_copy))
        fold.append(dataset_copy.pop(index))
        foldlabel.append(labels_copy.pop(index))
      dataset_split.append(fold)
      labels_split.append(foldlabel)
    return dataset_split,labels_split

  folds,labelfolds = kfold_split(data, labels, 5)

  (train_samples1)=np.array(folds[0]+folds[1]+folds[2]+folds[3])
  train_labels1=np.array(labelfolds[0]+labelfolds[1]+labelfolds[2]+labelfolds[3])
  test_sample1 = np.array(folds[4])
  test_label1=np.array(labelfolds[4])

  train_samples2=np.array(folds[0]+folds[1]+folds[2]+folds[4])
  train_labels2=np.array(labelfolds[0]+labelfolds[1]+labelfolds[2]+labelfolds[4])
  test_sample2=np.array(folds[3])
  test_label2=np.array(labelfolds[3])

  train_samples3=np.array(folds[0]+folds[1]+folds[3]+folds[4])
  train_labels3=np.array(labelfolds[0]+labelfolds[1]+labelfolds[3]+labelfolds[4])
  test_sample3=np.array(folds[2])
  test_label3=np.array(labelfolds[2])

  train_samples4=np.array(folds[0]+folds[2]+folds[3]+folds[4])
  train_labels4=np.array(labelfolds[0]+labelfolds[2]+labelfolds[3]+labelfolds[4])
  test_sample4=np.array(folds[1])
  test_label4=np.array(labelfolds[1])

  train_samples5= np.array(folds[1]+folds[2]+folds[3]+folds[4])
  train_labels5= np.array(labelfolds[1]+labelfolds[2]+labelfolds[3]+labelfolds[4])
  test_sample5 = np.array(folds[0])
  test_label5= np.array(labelfolds[0])

  def logreg(train_samples, train_labels, test_sample, test_label):

    logreg_obj = LogRegression(train_samples)
    theta, intercept, loss_train, acc_train = logreg_obj.fit(train_samples, train_labels, 0, 0, 1, 0)
    theta_test, intercept_test, loss_test, acc_test = logreg_obj.fit(test_sample, test_label, 0, 0, 1, 0)
    labels_predict_test = logreg_obj.predict(test_sample)
    labels_predict_train = logreg_obj.predict(train_samples)
    
    return loss_train, loss_test, acc_train, acc_test, theta, intercept, theta_test, intercept_test, labels_predict_train, labels_predict_test

  # Commented to check the saved file
  # loss1_train, loss1_test, acc1_train, acc1_test, w1_tr, b1_tr, w1_te, b1_te, y_predtr1, y_predte1 = logreg(train_samples1, train_labels1, test_sample1, test_label1)
  # loss2_train, loss2_test, acc2_train, acc2_test, w2_tr, b2_tr, w2_te, b2_te, y_predtr2, y_predte2 = logreg(train_samples2, train_labels2, test_sample2, test_label2)
  # loss3_train, loss3_test, acc3_train, acc3_test, w3_tr, b3_tr, w3_te, b3_te, y_predtr3, y_predte3 = logreg(train_samples3, train_labels3, test_sample3, test_label3)
  # loss4_train, loss4_test, acc4_train, acc4_test, w4_tr, b4_tr, w4_te, b4_te, y_predtr4, y_predte4 = logreg(train_samples4, train_labels4, test_sample4, test_label4)
  # loss5_train, loss5_test, acc5_train, acc5_test, w5_tr, b5_tr, w5_te, b5_te, y_predtr5, y_predte5 = logreg(train_samples5, train_labels5, test_sample5, test_label5)
  # print( acc1_train, acc1_test)

  # model = [loss1_train, loss1_test, acc1_train, acc1_test, w1_tr, b1_tr, w1_te, b1_te, y_predtr1, y_predte1,
  #          loss2_train, loss2_test, acc2_train, acc2_test, w2_tr, b2_tr, w2_te, b2_te, y_predtr2, y_predte2,
  #          loss3_train, loss3_test, acc3_train, acc3_test, w3_tr, b3_tr, w3_te, b3_te, y_predtr3, y_predte3,
  #          loss4_train, loss4_test, acc4_train, acc4_test, w4_tr, b4_tr, w4_te, b4_te, y_predtr4, y_predte4,
  #          loss5_train, loss5_test, acc5_train, acc5_test, w5_tr, b5_tr, w5_te, b5_te, y_predtr5, y_predte5]

  # # Save the model as a pickle in a file 
  # joblib.dump(model , '/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2c.sav') 
    
  # Load the model from the file 
  model_from_joblib = joblib.load('/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2c.sav')  
    
  loss1_train = model_from_joblib[0]
  loss1_test = model_from_joblib[1]
  acc1_train = model_from_joblib[2]
  acc1_test = model_from_joblib[3]
  loss2_train = model_from_joblib[10]
  loss2_test = model_from_joblib[11]
  acc2_train = model_from_joblib[12]
  acc2_test = model_from_joblib[13]
  loss3_train = model_from_joblib[20]
  loss3_test = model_from_joblib[21]
  acc3_train = model_from_joblib[22]
  acc3_test = model_from_joblib[23]
  loss4_train = model_from_joblib[30]
  loss4_test = model_from_joblib[31]
  acc4_train = model_from_joblib[32]
  acc4_test = model_from_joblib[33]
  loss5_train = model_from_joblib[40]
  loss5_test = model_from_joblib[41]
  acc5_train = model_from_joblib[42]
  acc5_test = model_from_joblib[43]

  iterations = (0, 200, 400, 600, 800, 1000)
  acc_table1 = pd.DataFrame(columns=['Model Number','Iterations', 'Train Accuracy','Test Accuracy', 'Train Loss', 'Test Loss'])
  pd.options.display.max_columns = None
  for i in range(0,6):

    #add iterations column
    new_row1 = {'Model Number':'1','Iterations': iterations[i], 'Train Accuracy':acc1_train[i] ,'Test Accuracy':acc1_test[i], 'Train Loss':loss1_train[i] ,'Test Loss':loss1_test[i]}
    acc_table1 = acc_table1.append(new_row1,ignore_index=True)
    new_row2 = {'Model Number':'2','Iterations': iterations[i],'Train Accuracy':acc2_train[i] ,'Test Accuracy':acc2_test[i], 'Train Loss':loss2_train[i] ,'Test Loss':loss2_test[i]}
    acc_table1 = acc_table1.append(new_row2,ignore_index=True)
    new_row3 = {'Model Number':'3','Iterations': iterations[i],'Train Accuracy':acc3_train[i] ,'Test Accuracy':acc3_test[i], 'Train Loss':loss3_train[i] ,'Test Loss':loss3_test[i]}
    acc_table1 = acc_table1.append(new_row3,ignore_index=True)
    new_row4 = {'Model Number':'4','Iterations': iterations[i],'Train Accuracy':acc4_train[i] ,'Test Accuracy':acc4_test[i], 'Train Loss':loss4_train[i] ,'Test Loss':loss4_test[i]}
    acc_table1 = acc_table1.append(new_row4,ignore_index=True)
    new_row5 = {'Model Number':'5','Iterations': iterations[i],'Train Accuracy':acc5_train[i] ,'Test Accuracy':acc5_test[i], 'Train Loss':loss5_train[i] ,'Test Loss':loss5_test[i]}
    acc_table1 = acc_table1.append(new_row5,ignore_index=True)

  display(acc_table1) 


  fig = plt.figure()
  fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(20, 20))

  axs[0, 0].plot(iterations, loss1_train, label = "Training Loss")
  axs[0, 0].plot(iterations, loss1_test, label = "Test Loss" )
  axs[0, 0].set_xlabel("Iterations")
  axs[0, 0].set_ylabel("Loss")
  axs[0, 0].legend(loc="best")
  axs[0, 0].set_title("Model 1")
  axs[0, 1].plot(iterations, acc1_train, label = "Training Accuracy")
  axs[0, 1].plot(iterations, acc1_test, label = "Test Accuracy")
  axs[0, 1].set_xlabel("Iterations")
  axs[0, 1].set_ylabel("Accuracy")
  axs[0, 1].legend(loc= "best")
  axs[0, 1].set_title("Model 1")

  axs[1, 0].plot(iterations, loss2_train, label = "Training Loss")
  axs[1, 0].plot(iterations, loss2_test, label = "Test Loss" )
  axs[1, 0].set_xlabel("Iterations")
  axs[1, 0].set_ylabel("Loss")
  axs[1, 0].legend(loc="best")
  axs[1, 0].set_title("Model 2")
  axs[1, 1].plot(iterations, acc2_train, label = "Training Accuracy")
  axs[1, 1].plot(iterations, acc2_test, label = "Test Accuracy")
  axs[1, 1].set_xlabel("Iterations")
  axs[1, 1].set_ylabel("Accuracy")
  axs[1, 1].legend(loc= "best")
  axs[1, 1].set_title("Model 2")

  axs[2, 0].plot(iterations, loss3_train, label = "Training Loss")
  axs[2, 0].plot(iterations, loss3_test, label = "Test Loss" )
  axs[2, 0].set_xlabel("Iterations")
  axs[2, 0].set_ylabel("Loss")
  axs[2, 0].legend(loc="best")
  axs[2, 0].set_title("Model 3")
  axs[2, 1].plot(iterations, acc3_train, label = "Training Accuracy")
  axs[2, 1].plot(iterations, acc3_test, label = "Test Accuracy")
  axs[2, 1].set_xlabel("Iterations")
  axs[2, 1].set_ylabel("Accuracy")
  axs[2, 1].legend(loc= "best")
  axs[2, 1].set_title("Model 3")

  axs[3, 0].plot(iterations, loss4_train, label = "Training Loss")
  axs[3, 0].plot(iterations, loss4_test, label = "Test Loss" )
  axs[3, 0].set_xlabel("Iterations")
  axs[3, 0].set_ylabel("Loss")
  axs[3, 0].legend(loc="best")
  axs[3, 0].set_title("Model 4")
  axs[3, 1].plot(iterations, acc4_train, label = "Training Accuracy")
  axs[3, 1].plot(iterations, acc4_test, label = "Test Accuracy")
  axs[3, 1].set_xlabel("Iterations")
  axs[3, 1].set_ylabel("Accuracy")
  axs[3, 1].legend(loc= "best")
  axs[3, 1].set_title("Model 4")


  axs[4, 0].plot(iterations, loss5_train, label = "Training Loss")
  axs[4, 0].plot(iterations, loss5_test, label = "Test Loss" )
  axs[4, 0].set_xlabel("Iterations")
  axs[4, 0].set_ylabel("Loss")
  axs[4, 0].legend(loc="best")
  axs[4, 0].set_title("Model 5")
  axs[4, 1].plot(iterations, acc5_train, label = "Training Accuracy")
  axs[4, 1].plot(iterations, acc5_test, label = "Test Accuracy")
  axs[4, 1].set_xlabel("Iterations")
  axs[4, 1].set_ylabel("Accuracy")
  axs[4, 1].legend(loc= "best")
  axs[4, 1].set_title("Model 5")


  fig.suptitle('Loss and Accuracy Plots for all 5 models') 
  fig. tight_layout(pad=5.0)
  plt.show() 
q2a()
q2c()

"""**2 Part d**"""

def q2d():
  # Split a dataset into k folds
  def kfold_split(dataset,labels,folds):
    dataset_split=list()
    labels_split=list()
    dataset_copy=dataset.tolist()
    labels_copy=labels.tolist()
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
      fold=list()
      foldlabel=list()
      while len(fold)<fold_size:
        index = randrange(len(dataset_copy))
        #p=np.random.permutation(len(dataset_copy))
        fold.append(dataset_copy.pop(index))
        foldlabel.append(labels_copy.pop(index))
      dataset_split.append(fold)
      labels_split.append(foldlabel)
    return dataset_split,labels_split

  folds,labelfolds = kfold_split(data, labels, 5)

  (train_samples1)=np.array(folds[0]+folds[1]+folds[2]+folds[3])
  train_labels1=np.array(labelfolds[0]+labelfolds[1]+labelfolds[2]+labelfolds[3])
  test_sample1 = np.array(folds[4])
  test_label1=np.array(labelfolds[4])

  train_samples2=np.array(folds[0]+folds[1]+folds[2]+folds[4])
  train_labels2=np.array(labelfolds[0]+labelfolds[1]+labelfolds[2]+labelfolds[4])
  test_sample2=np.array(folds[3])
  test_label2=np.array(labelfolds[3])

  train_samples3=np.array(folds[0]+folds[1]+folds[3]+folds[4])
  train_labels3=np.array(labelfolds[0]+labelfolds[1]+labelfolds[3]+labelfolds[4])
  test_sample3=np.array(folds[2])
  test_label3=np.array(labelfolds[2])

  train_samples4=np.array(folds[0]+folds[2]+folds[3]+folds[4])
  train_labels4=np.array(labelfolds[0]+labelfolds[2]+labelfolds[3]+labelfolds[4])
  test_sample4=np.array(folds[1])
  test_label4=np.array(labelfolds[1])

  train_samples5= np.array(folds[1]+folds[2]+folds[3]+folds[4])
  train_labels5= np.array(labelfolds[1]+labelfolds[2]+labelfolds[3]+labelfolds[4])
  test_sample5 = np.array(folds[0])
  test_label5= np.array(labelfolds[0])



  def logregwithreg(train_samples, train_labels, test_sample, test_label):

      
      #reg_const = [0.01, 0.02, 0.03, 0.1, 0.2, 0.3]
      reg_const = [0.1, 0.2, 0.3, 1, 5, 10]
      loss_trlst = []
      loss_telst = []
      acc_trlst = []
      acc_telst = []
      for i in reg_const:

          logreg = LogRegression(train_samples)
          theta, intercept, loss_train, acc_train = logreg.fit(train_samples, train_labels, 1, i, 1, 0)
          theta_test, intercept_test, loss_test, acc_test = logreg.fit(test_sample, test_label, 1, i, 1, 0)
          labels_predict_test = logreg.predict(test_sample)
          labels_predict_train = logreg.predict(train_samples)
          loss_trlst.append(loss_train[5])
          loss_telst.append(loss_test[5])
          acc_trlst.append(acc_train[5])
          acc_telst.append(acc_test[5])

      return loss_trlst, loss_telst, acc_trlst, acc_telst, theta, intercept, theta_test, intercept_test, labels_predict_train, labels_predict_test

  #commented to check the saved models
  # loss1_trlst, loss1_telst, acc1_trlst, acc1_telst, w1reg_tr, b1reg_tr, w1reg_te, b1reg_te, y_ptr1, y_prte1 = logregwithreg(train_samples1,train_labels1,test_sample1,test_label1)
  # loss2_trlst, loss2_telst, acc2_trlst, acc2_telst, w2reg_tr, b2reg_tr, w2reg_te, b2reg_te, y_ptr2, y_prte2 = logregwithreg(train_samples2,train_labels2,test_sample2,test_label2)
  # loss3_trlst, loss3_telst, acc3_trlst, acc3_telst, w3reg_tr, b3reg_tr, w3reg_te, b3reg_te, y_ptr3, y_prte3 = logregwithreg(train_samples3,train_labels3,test_sample3,test_label3)
  # loss4_trlst, loss4_telst, acc4_trlst, acc4_telst, w4reg_tr, b4reg_tr, w4reg_te, b4reg_te, y_ptr4, y_prte4 = logregwithreg(train_samples4,train_labels4,test_sample4,test_label4)
  # loss5_trlst, loss5_telst, acc5_trlst, acc5_telst, w5reg_tr, b5reg_tr, w5reg_te, b5reg_te, y_ptr5, y_prte5 = logregwithreg(train_samples5,train_labels5,test_sample5,test_label5)
  # #print(acc1_trlst, acc1_telst)

  # model2 = [loss1_trlst, loss1_telst, acc1_trlst, acc1_telst, w1reg_tr, b1reg_tr, w1reg_te, b1reg_te, y_ptr1, y_prte1,
  #          loss2_trlst, loss2_telst, acc2_trlst, acc2_telst, w2reg_tr, b2reg_tr, w2reg_te, b2reg_te, y_ptr2, y_prte2,
  #          loss3_trlst, loss3_telst, acc3_trlst, acc3_telst, w3reg_tr, b3reg_tr, w3reg_te, b3reg_te, y_ptr3, y_prte3,
  #          loss4_trlst, loss4_telst, acc4_trlst, acc4_telst, w4reg_tr, b4reg_tr, w4reg_te, b4reg_te, y_ptr4, y_prte4,
  #          loss5_trlst, loss5_telst, acc5_trlst, acc5_telst, w5reg_tr, b5reg_tr, w5reg_te, b5reg_te, y_ptr5, y_prte5]

  # # Save the model as a pickle in a file 
  # joblib.dump(model2 , '/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2d1.sav') 
    
  # Load the model from the file 
  model_from_joblib2 = joblib.load('/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2d1.sav')  
    
  loss1_trlst = model_from_joblib2[0]
  loss1_telst = model_from_joblib2[1]
  acc1_trlst = model_from_joblib2[2]
  acc1_telst = model_from_joblib2[3]
  loss2_trlst = model_from_joblib2[10]
  loss2_telst = model_from_joblib2[11]
  acc2_trlst = model_from_joblib2[12]
  acc2_telst = model_from_joblib2[13]
  loss3_trlst = model_from_joblib2[20]
  loss3_telst = model_from_joblib2[21]
  acc3_trlst = model_from_joblib2[22]
  acc3_telst = model_from_joblib2[23]
  loss4_trlst = model_from_joblib2[30]
  loss4_telst = model_from_joblib2[31]
  acc4_trlst = model_from_joblib2[32]
  acc4_telst = model_from_joblib2[33]
  loss5_trlst = model_from_joblib2[40]
  loss5_telst = model_from_joblib2[41]
  acc5_trlst = model_from_joblib2[42]
  acc5_telst = model_from_joblib2[43]


  acc_table2 = pd.DataFrame(columns=['Model Number','Lambda', 'Iterations','Train Accuracy','Test Accuracy', 'Train Loss', 'Test Loss'])
  iterations = (0, 200, 400, 600, 800, 1000)
  reg_constant = [0.1, 0.2, 0.3, 1, 5, 10]

  for i in range(0,6):

    row1 = {'Model Number':'1','Lambda':reg_constant[i], 'Iterations': '1000','Train Accuracy':acc1_trlst[i] ,'Test Accuracy':acc1_telst[i], 'Train Loss':loss1_trlst[i] ,'Test Loss':loss1_telst[i]}
    acc_table2 = acc_table2.append(row1,ignore_index=True)
    row2 = {'Model Number':'2','Lambda':reg_constant[i], 'Iterations': '1000','Train Accuracy':acc2_trlst[i] ,'Test Accuracy':acc2_telst[i], 'Train Loss':loss2_trlst[i] ,'Test Loss':loss2_telst[i]}
    acc_table2 = acc_table2.append(row2,ignore_index=True)
    row3 = {'Model Number':'3','Lambda':reg_constant[i], 'Iterations': '1000','Train Accuracy':acc3_trlst[i] ,'Test Accuracy':acc3_telst[i], 'Train Loss':loss3_trlst[i] ,'Test Loss':loss3_telst[i]}
    acc_table2 = acc_table2.append(row3,ignore_index=True)
    row4 = {'Model Number':'4','Lambda':reg_constant[i], 'Iterations': '1000','Train Accuracy':acc4_trlst[i] ,'Test Accuracy':acc4_telst[i], 'Train Loss':loss4_trlst[i] ,'Test Loss':loss4_telst[i]}
    acc_table2 = acc_table2.append(row4,ignore_index=True)
    row5 = {'Model Number':'5','Lambda':reg_constant[i], 'Iterations': '1000','Train Accuracy':acc5_trlst[i] ,'Test Accuracy':acc5_telst[i], 'Train Loss':loss5_trlst[i] ,'Test Loss':loss5_telst[i]}
    acc_table2 = acc_table2.append(row5,ignore_index=True)
  display(acc_table2)



  def logregwithreg_sec(train_samples, train_labels, test_sample, test_label):

      logreg = LogRegression(train_samples)
      theta, intercept, loss_train, acc_train = logreg.fit(train_samples, train_labels, 1, 0.1, 1, 0)
      theta_test, intercept_test, loss_test, acc_test = logreg.fit(test_sample, test_label, 1, 0.1, 1, 0)
      labels_predict_test = logreg.predict(test_sample)
      labels_predict_train = logreg.predict(train_samples)
      return loss_train, loss_test, acc_train, acc_test, theta, intercept, theta_test, intercept_test, labels_predict_train, labels_predict_train

  # Commented to check the saved file
  # loss1_trreg, loss1_tereg, acc1_trreg, acc1_tereg, wtr1, btr1, wte1, bte1, lbl_ptr1, lbl_prete1 = logregwithreg_sec(train_samples1, train_labels1, test_sample1, test_label1)
  # loss2_trreg, loss2_tereg, acc2_trreg, acc2_tereg, wtr2, btr2, wte2, bte2, lbl_ptr2, lbl_prete2 = logregwithreg_sec(train_samples2, train_labels2, test_sample2, test_label2)
  # loss3_trreg, loss3_tereg, acc3_trreg, acc3_tereg, wtr3, btr3, wte3, bte3, lbl_ptr3, lbl_prete3 = logregwithreg_sec(train_samples3, train_labels3, test_sample3, test_label3)
  # loss4_trreg, loss4_tereg, acc4_trreg, acc4_tereg, wtr4, btr4, wte4, bte4, lbl_ptr4, lbl_prete4 = logregwithreg_sec(train_samples4, train_labels4, test_sample4, test_label4)
  # loss5_trreg, loss5_tereg, acc5_trreg, acc5_tereg, wtr5, btr5, wte5, bte5, lbl_ptr5, lbl_prete5 = logregwithreg_sec(train_samples5, train_labels5, test_sample5, test_label5)


  # model3 = [loss1_trreg, loss1_tereg, acc1_trreg, acc1_tereg, wtr1, btr1, wte1, bte1, lbl_ptr1, lbl_prete1,
  #          loss2_trreg, loss2_tereg, acc2_trreg, acc2_tereg, wtr2, btr2, wte2, bte2, lbl_ptr2, lbl_prete2,
  #          loss3_trreg, loss3_tereg, acc3_trreg, acc3_tereg, wtr3, btr3, wte3, bte3, lbl_ptr3, lbl_prete3,
  #          loss4_trreg, loss4_tereg, acc4_trreg, acc4_tereg, wtr4, btr4, wte4, bte4, lbl_ptr4, lbl_prete4,
  #          loss5_trreg, loss5_tereg, acc5_trreg, acc5_tereg, wtr5, btr5, wte5, bte5, lbl_ptr5, lbl_prete5]

  # #Save the model as a pickle in a file 
  # joblib.dump(model3 , '/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2d2.sav') 
    
  # Load the model from the file 
  model_from_joblib3 = joblib.load('/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2d2.sav')  
    
  loss1_trreg = model_from_joblib3[0]
  loss1_tereg = model_from_joblib3[1]
  acc1_trreg = model_from_joblib3[2]
  acc1_tereg = model_from_joblib3[3]
  loss2_trreg = model_from_joblib3[10]
  loss2_tereg = model_from_joblib3[11]
  acc2_trreg = model_from_joblib3[12]
  acc2_tereg = model_from_joblib3[13]
  loss3_trreg = model_from_joblib3[20]
  loss3_tereg = model_from_joblib3[21]
  acc3_trreg = model_from_joblib3[22]
  acc3_tereg = model_from_joblib3[23]
  loss4_trreg = model_from_joblib3[30]
  loss4_tereg = model_from_joblib3[31]
  acc4_trreg = model_from_joblib3[32]
  acc4_tereg = model_from_joblib3[33]
  loss5_trreg = model_from_joblib3[40]
  loss5_tereg = model_from_joblib3[41]
  acc5_trreg = model_from_joblib3[42]
  acc5_tereg = model_from_joblib3[43]




  acc_table3 = pd.DataFrame(columns=['Model Number','Iterations', 'Train Accuracy','Test Accuracy', 'Train Loss', 'Test Loss'])

  for i in range(0,6):

    #add iterations column
    new_row1 = {'Model Number':'1','Iterations': iterations[i], 'Train Accuracy':acc1_trreg[i] ,'Test Accuracy':acc1_tereg[i], 'Train Loss':loss1_trreg[i] ,'Test Loss':loss1_tereg[i]}
    acc_table3 = acc_table3.append(new_row1,ignore_index=True)
    new_row2 = {'Model Number':'2','Iterations': iterations[i],'Train Accuracy':acc2_trreg[i] ,'Test Accuracy':acc2_tereg[i], 'Train Loss':loss2_trreg[i] ,'Test Loss':loss2_tereg[i]}
    acc_table3 = acc_table3.append(new_row2,ignore_index=True)
    new_row3 = {'Model Number':'3','Iterations': iterations[i],'Train Accuracy':acc3_trreg[i] ,'Test Accuracy':acc3_tereg[i], 'Train Loss':loss3_trreg[i] ,'Test Loss':loss3_tereg[i]}
    acc_table3 = acc_table3.append(new_row3,ignore_index=True)
    new_row4 = {'Model Number':'4','Iterations': iterations[i],'Train Accuracy':acc4_trreg[i] ,'Test Accuracy':acc4_tereg[i], 'Train Loss':loss4_trreg[i] ,'Test Loss':loss4_tereg[i]}
    acc_table3 = acc_table3.append(new_row4,ignore_index=True)
    new_row5 = {'Model Number':'5','Iterations': iterations[i],'Train Accuracy':acc5_trreg[i] ,'Test Accuracy':acc5_tereg[i], 'Train Loss':loss5_trreg[i] ,'Test Loss':loss5_tereg[i]}
    acc_table3 = acc_table3.append(new_row5,ignore_index=True)

  display(acc_table3)

  fig = plt.figure()
  fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(20,20))

  axs[0, 0].plot(iterations, loss1_trreg, label = "Training Loss")
  axs[0, 0].plot(iterations, loss1_tereg, label = "Test Loss" )
  axs[0, 0].set_xlabel("Iterations")
  axs[0, 0].set_ylabel("Loss")
  axs[0, 0].legend(loc="best")
  axs[0, 0].set_title("Model 1")
  axs[0, 1].plot(iterations, acc1_trreg, label = "Training Accuracy")
  axs[0, 1].plot(iterations, acc1_tereg, label = "Test Accuracy")
  axs[0, 1].set_xlabel("Iterations")
  axs[0, 1].set_ylabel("Accuracy")
  axs[0, 1].legend(loc= "best")
  axs[0, 1].set_title("Model 1")

  axs[1, 0].plot(iterations, loss2_trreg, label = "Training Loss")
  axs[1, 0].plot(iterations, loss2_tereg, label = "Test Loss" )
  axs[1, 0].set_xlabel("Iterations")
  axs[1, 0].set_ylabel("Loss")
  axs[1, 0].legend(loc="best")
  axs[1, 0].set_title("Model 2")
  axs[1, 1].plot(iterations, acc2_trreg, label = "Training Accuracy")
  axs[1, 1].plot(iterations, acc2_tereg, label = "Test Accuracy")
  axs[1, 1].set_xlabel("Iterations")
  axs[1, 1].set_ylabel("Accuracy")
  axs[1, 1].legend(loc= "best")
  axs[1, 1].set_title("Model 2")

  axs[2, 0].plot(iterations, loss3_trreg, label = "Training Loss") 
  axs[2, 0].plot(iterations, loss3_tereg, label = "Test Loss" )
  axs[2, 0].set_xlabel("Iterations")
  axs[2, 0].set_ylabel("Loss")
  axs[2, 0].legend(loc="best")
  axs[2, 0].set_title("Model 3")
  axs[2, 1].plot(iterations, acc3_trreg, label = "Training Accuracy")
  axs[2, 1].plot(iterations, acc3_tereg, label = "Test Accuracy")
  axs[2, 1].set_xlabel("Iterations")
  axs[2, 1].set_ylabel("Accuracy")
  axs[2, 1].legend(loc= "best")
  axs[2, 1].set_title("Model 3")

  axs[3, 0].plot(iterations, loss4_trreg, label = "Training Loss")
  axs[3, 0].plot(iterations, loss4_tereg, label = "Test Loss" )
  axs[3, 0].set_xlabel("Iterations")
  axs[3, 0].set_ylabel("Loss")
  axs[3, 0].legend(loc="best")
  axs[3, 0].set_title("Model 4")
  axs[3, 1].plot(iterations, acc4_trreg, label = "Training Accuracy")
  axs[3, 1].plot(iterations, acc4_tereg, label = "Test Accuracy")
  axs[3, 1].set_xlabel("Iterations")
  axs[3, 1].set_ylabel("Accuracy")
  axs[3, 1].legend(loc= "best")
  axs[3, 1].set_title("Model 4")

  axs[4, 0].plot(iterations, loss5_trreg, label = "Training Loss")
  axs[4, 0].plot(iterations, loss5_tereg, label = "Test Loss" )
  axs[4, 0].set_xlabel("Iterations")
  axs[4, 0].set_ylabel("Loss")
  axs[4, 0].legend(loc="best")
  axs[4, 0].set_title("Model 5")
  axs[4, 1].plot(iterations, acc5_trreg, label = "Training Accuracy")
  axs[4, 1].plot(iterations, acc5_tereg, label = "Test Accuracy")
  axs[4, 1].set_xlabel("Iterations")
  axs[4, 1].set_ylabel("Accuracy")
  axs[4, 1].legend(loc= "best")
  axs[4, 1].set_title("Model 5")

  fig.suptitle('Loss and Accuracy Plots for all 5 models with regularisation constant lambda = 0.1') 
  fig. tight_layout(pad=5.0)
  plt.show() 

q2a()
q2c()
q2d()

"""**2 Part e**"""

def q2e():
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import log_loss
  from sklearn.model_selection import GridSearchCV
  import joblib

  # Split a dataset into k folds
  def kfold_split(dataset,labels,folds):
    dataset_split=list()
    labels_split=list()
    dataset_copy=dataset.tolist()
    labels_copy=labels.tolist()
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
      fold=list()
      foldlabel=list()
      while len(fold)<fold_size:
        index = randrange(len(dataset_copy))
        #p=np.random.permutation(len(dataset_copy))
        fold.append(dataset_copy.pop(index))
        foldlabel.append(labels_copy.pop(index))
      dataset_split.append(fold)
      labels_split.append(foldlabel)
    return dataset_split,labels_split

  folds,labelfolds = kfold_split(data, labels, 5)

  (train_samples1)=np.array(folds[0]+folds[1]+folds[2]+folds[3])
  train_labels1=np.array(labelfolds[0]+labelfolds[1]+labelfolds[2]+labelfolds[3])
  test_sample1 = np.array(folds[4])
  test_label1=np.array(labelfolds[4])

  train_samples2=np.array(folds[0]+folds[1]+folds[2]+folds[4])
  train_labels2=np.array(labelfolds[0]+labelfolds[1]+labelfolds[2]+labelfolds[4])
  test_sample2=np.array(folds[3])
  test_label2=np.array(labelfolds[3])

  train_samples3=np.array(folds[0]+folds[1]+folds[3]+folds[4])
  train_labels3=np.array(labelfolds[0]+labelfolds[1]+labelfolds[3]+labelfolds[4])
  test_sample3=np.array(folds[2])
  test_label3=np.array(labelfolds[2])

  train_samples4=np.array(folds[0]+folds[2]+folds[3]+folds[4])
  train_labels4=np.array(labelfolds[0]+labelfolds[2]+labelfolds[3]+labelfolds[4])
  test_sample4=np.array(folds[1])
  test_label4=np.array(labelfolds[1])

  train_samples5= np.array(folds[1]+folds[2]+folds[3]+folds[4])
  train_labels5= np.array(labelfolds[1]+labelfolds[2]+labelfolds[3]+labelfolds[4])
  test_sample5 = np.array(folds[0])
  test_label5= np.array(labelfolds[0])


  def skl_logreg(train_samples, train_labels, test_sample, test_label):
      #iterations = (0, 200, 400, 600, 800, 1000)
      accuracy_test_lst =[]
      accuracy_train_lst = []
      loss_train_lst = []
      loss_test_lst = []
      num_iters = 1000
      # for i in iterations:
      for i in range(num_iters+1):
          clf = LogisticRegression(max_iter = num_iters).fit(train_samples, train_labels)
          
          predicted_labels = clf.predict(test_sample)
          predicted_labels_train = clf.predict(train_samples)
          predicted_probab = clf.predict_proba(test_sample)
          predicted_probab_train = clf.predict_proba(train_samples)

          accuracy_test = clf.score(test_sample, test_label)
          loss_test = log_loss(test_label,predicted_probab)
          accuracy_train = clf.score(train_samples, train_labels)
          loss_train = log_loss(train_labels,predicted_probab_train)

          if i % 200 == 0:
              accuracy_test_lst.append(accuracy_test) 
              accuracy_train_lst.append(accuracy_train) 
              loss_test_lst.append(loss_test)
              loss_train_lst.append(loss_train)
              # print(clf.coef_)
              # print(clf.intercept_)
      return loss_train_lst, loss_test_lst, accuracy_train_lst, accuracy_test_lst, clf

  

  #commented to check the saved file data
  # loss1_tr_skl, loss1_te_skl, acc1_tr_skl, acc1_te_skl, clf1 = skl_logreg(train_samples1, train_labels1, test_sample1, test_label1)
  # loss2_tr_skl, loss2_te_skl, acc2_tr_skl, acc2_te_skl, clf2 = skl_logreg(train_samples2, train_labels2, test_sample2, test_label2)
  # loss3_tr_skl, loss3_te_skl, acc3_tr_skl, acc3_te_skl, clf3 = skl_logreg(train_samples3, train_labels3, test_sample3, test_label3)
  # loss4_tr_skl, loss4_te_skl, acc4_tr_skl, acc4_te_skl, clf4 = skl_logreg(train_samples4, train_labels4, test_sample4, test_label4)
  # loss5_tr_skl, loss5_te_skl, acc5_tr_skl, acc5_te_skl, clf5 = skl_logreg(train_samples5, train_labels5, test_sample5, test_label5)


  # model4 = [loss1_tr_skl, loss1_te_skl, acc1_tr_skl, acc1_te_skl, clf1,
  #          loss2_tr_skl, loss2_te_skl, acc2_tr_skl, acc2_te_skl, clf2,
  #          loss3_tr_skl, loss3_te_skl, acc3_tr_skl, acc3_te_skl, clf3,
  #          loss4_tr_skl, loss4_te_skl, acc4_tr_skl, acc4_te_skl, clf4,
  #          loss5_tr_skl, loss5_te_skl, acc5_tr_skl, acc5_te_skl, clf5]

  # #Save the model as a pickle in a file 
  # joblib.dump(model4 , '/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2e1.sav') 
    
  # Load the model from the file 
  model_from_joblib4 = joblib.load('/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2e1.sav')  
    
  loss1_tr_skl = model_from_joblib4[0]
  loss1_te_skl = model_from_joblib4[1]
  acc1_tr_skl = model_from_joblib4[2]
  acc1_te_skl = model_from_joblib4[3]
  loss2_tr_skl = model_from_joblib4[5]
  loss2_te_skl = model_from_joblib4[6]
  acc2_tr_skl = model_from_joblib4[7]
  acc2_te_skl = model_from_joblib4[8]
  loss3_tr_skl = model_from_joblib4[10]
  loss3_te_skl = model_from_joblib4[11]
  acc3_tr_skl = model_from_joblib4[12]
  acc3_te_skl = model_from_joblib4[13]
  loss4_tr_skl = model_from_joblib4[15]
  loss4_te_skl = model_from_joblib4[16]
  acc4_tr_skl = model_from_joblib4[17]
  acc4_te_skl = model_from_joblib4[18]
  loss5_tr_skl = model_from_joblib4[20]
  loss5_te_skl = model_from_joblib4[21]
  acc5_tr_skl = model_from_joblib4[22]
  acc5_te_skl = model_from_joblib4[23]


  #print(acc1_tr_skl, acc1_te_skl)
  acc_table4_skl = pd.DataFrame(columns=['Model Number','Iterations', 'Train Accuracy','Test Accuracy', 'Train Loss', 'Test Loss'])

  for i in range(0,6):

    #add iterations column
    new_row1_skl = {'Model Number':'1','Iterations': iterations[i], 'Train Accuracy':acc1_tr_skl[i] ,'Test Accuracy':acc1_te_skl[i], 'Train Loss':loss1_tr_skl[i] ,'Test Loss':loss1_te_skl[i]}
    acc_table4_skl = acc_table4_skl.append(new_row1_skl,ignore_index=True)
    new_row2_skl = {'Model Number':'2','Iterations': iterations[i],'Train Accuracy':acc2_tr_skl[i] ,'Test Accuracy':acc2_te_skl[i], 'Train Loss':loss2_tr_skl[i] ,'Test Loss':loss2_te_skl[i]}
    acc_table4_skl = acc_table4_skl.append(new_row2_skl,ignore_index=True)
    new_row3_skl = {'Model Number':'3','Iterations': iterations[i],'Train Accuracy':acc3_tr_skl[i] ,'Test Accuracy':acc3_te_skl[i], 'Train Loss':loss3_tr_skl[i] ,'Test Loss':loss3_te_skl[i]}
    acc_table4_skl = acc_table4_skl.append(new_row3_skl,ignore_index=True)
    new_row4_skl = {'Model Number':'4','Iterations': iterations[i],'Train Accuracy':acc4_tr_skl[i] ,'Test Accuracy':acc4_te_skl[i], 'Train Loss':loss4_tr_skl[i] ,'Test Loss':loss4_te_skl[i]}
    acc_table4_skl = acc_table4_skl.append(new_row4_skl,ignore_index=True)
    new_row5_skl = {'Model Number':'5','Iterations': iterations[i],'Train Accuracy':acc5_tr_skl[i] ,'Test Accuracy':acc5_te_skl[i], 'Train Loss':loss5_tr_skl[i] ,'Test Loss':loss5_te_skl[i]}
    acc_table4_skl = acc_table4_skl.append(new_row5_skl,ignore_index=True)

  display(acc_table4_skl)

  fig = plt.figure()
  fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(20,20))

  axs[0, 0].plot(iterations, loss1_tr_skl, label = "Training Loss")
  axs[0, 0].plot(iterations, loss1_te_skl, label = "Test Loss" )
  axs[0, 0].set_xlabel("Iterations")
  axs[0, 0].set_ylabel("Loss")
  axs[0, 0].legend(loc="best")
  axs[0, 0].set_title("Model 1")
  axs[0, 1].plot(iterations, acc1_tr_skl, label = "Training Accuracy")
  axs[0, 1].plot(iterations, acc1_te_skl, label = "Test Accuracy")
  axs[0, 1].set_xlabel("Iterations")
  axs[0, 1].set_ylabel("Accuracy")
  axs[0, 1].legend(loc= "best")
  axs[0, 1].set_title("Model 1")

  axs[1, 0].plot(iterations, loss2_tr_skl, label = "Training Loss")
  axs[1, 0].plot(iterations, loss2_te_skl, label = "Test Loss" )
  axs[1, 0].set_xlabel("Iterations")
  axs[1, 0].set_ylabel("Loss")
  axs[1, 0].legend(loc="best")
  axs[1, 0].set_title("Model 2")
  axs[1, 1].plot(iterations, acc2_tr_skl, label = "Training Accuracy")
  axs[1, 1].plot(iterations, acc2_te_skl, label = "Test Accuracy")
  axs[1, 1].set_xlabel("Iterations")
  axs[1, 1].set_ylabel("Accuracy")
  axs[1, 1].legend(loc= "best")
  axs[1, 1].set_title("Model 2")

  axs[2, 0].plot(iterations, loss3_tr_skl, label = "Training Loss") 
  axs[2, 0].plot(iterations, loss3_te_skl, label = "Test Loss" )
  axs[2, 0].set_xlabel("Iterations")
  axs[2, 0].set_ylabel("Loss")
  axs[2, 0].legend(loc="best")
  axs[2, 0].set_title("Model 3")
  axs[2, 1].plot(iterations, acc3_tr_skl, label = "Training Accuracy")
  axs[2, 1].plot(iterations, acc3_te_skl, label = "Test Accuracy")
  axs[2, 1].set_xlabel("Iterations")
  axs[2, 1].set_ylabel("Accuracy")
  axs[2, 1].legend(loc= "best")
  axs[2, 1].set_title("Model 3")

  axs[3, 0].plot(iterations, loss4_tr_skl, label = "Training Loss")
  axs[3, 0].plot(iterations, loss4_te_skl, label = "Test Loss" )
  axs[3, 0].set_xlabel("Iterations")
  axs[3, 0].set_ylabel("Loss")
  axs[3, 0].legend(loc="best")
  axs[3, 0].set_title("Model 4")
  axs[3, 1].plot(iterations, acc4_tr_skl, label = "Training Accuracy")
  axs[3, 1].plot(iterations, acc4_te_skl, label = "Test Accuracy")
  axs[3, 1].set_xlabel("Iterations")
  axs[3, 1].set_ylabel("Accuracy")
  axs[3, 1].legend(loc= "best")
  axs[3, 1].set_title("Model 4")

  axs[4, 0].plot(iterations, loss5_tr_skl, label = "Training Loss")
  axs[4, 0].plot(iterations, loss5_te_skl, label = "Test Loss" )
  axs[4, 0].set_xlabel("Iterations")
  axs[4, 0].set_ylabel("Loss")
  axs[4, 0].legend(loc="best")
  axs[4, 0].set_title("Model 5")
  axs[4, 1].plot(iterations, acc5_tr_skl, label = "Training Accuracy")
  axs[4, 1].plot(iterations, acc5_te_skl, label = "Test Accuracy")
  axs[4, 1].set_xlabel("Iterations")
  axs[4, 1].set_ylabel("Accuracy")
  axs[4, 1].legend(loc= "best")
  axs[4, 1].set_title("Model 5")

  fig.suptitle('Loss and Accuracy Plots for all 5 models') 
  fig. tight_layout(pad=5.0)
  plt.show()

  #with regularisation
  def skl_logreg_withreg(train_samples, train_labels, test_sample, test_label):
      #iterations = (0, 200, 400, 600, 800, 1000)
      accuracy_test_lst =[]
      accuracy_train_lst = []
      loss_train_lst = []
      loss_test_lst = []
      num_iters = 1000
      # for i in iterations:
      for i in range(num_iters+1):
          clf = LogisticRegression(max_iter = num_iters, C=10).fit(train_samples, train_labels)
          
          predicted_labels = clf.predict(test_sample)
          predicted_labels_train = clf.predict(train_samples)
          predicted_probab = clf.predict_proba(test_sample)
          predicted_probab_train = clf.predict_proba(train_samples)

          accuracy_test = clf.score(test_sample, test_label)
          loss_test = log_loss(test_label,predicted_probab)
          accuracy_train = clf.score(train_samples, train_labels)
          loss_train = log_loss(train_labels,predicted_probab_train)

          if i % 200 == 0:
              accuracy_test_lst.append(accuracy_test) 
              accuracy_train_lst.append(accuracy_train) 
              loss_test_lst.append(loss_test)
              loss_train_lst.append(loss_train)
              # print(clf.coef_)
              # print(clf.intercept_)
      return loss_train_lst, loss_test_lst, accuracy_train_lst, accuracy_test_lst, clf

  #commented to check the saved file data
  # loss1_trreg_skl, loss1_tereg_skl, acc1_trreg_skl, acc1_tereg_skl, clf1e2 = skl_logreg_withreg(train_samples1, train_labels1, test_sample1, test_label1)
  # loss2_trreg_skl, loss2_tereg_skl, acc2_trreg_skl, acc2_tereg_skl, clf2e2 = skl_logreg_withreg(train_samples2, train_labels2, test_sample2, test_label2)
  # loss3_trreg_skl, loss3_tereg_skl, acc3_trreg_skl, acc3_tereg_skl, clf3e2 = skl_logreg_withreg(train_samples3, train_labels3, test_sample3, test_label3)
  # loss4_trreg_skl, loss4_tereg_skl, acc4_trreg_skl, acc4_tereg_skl, clf4e2 = skl_logreg_withreg(train_samples4, train_labels4, test_sample4, test_label4)
  # loss5_trreg_skl, loss5_tereg_skl, acc5_trreg_skl, acc5_tereg_skl, clf5e2 = skl_logreg_withreg(train_samples5, train_labels5, test_sample5, test_label5)


  # model5 = [loss1_trreg_skl, loss1_tereg_skl, acc1_trreg_skl, acc1_tereg_skl, clf1e2,
  #          loss2_trreg_skl, loss2_tereg_skl, acc2_trreg_skl, acc2_tereg_skl, clf2e2,
  #          loss3_trreg_skl, loss3_tereg_skl, acc3_trreg_skl, acc3_tereg_skl, clf3e2,
  #          loss4_trreg_skl, loss4_tereg_skl, acc4_trreg_skl, acc4_tereg_skl, clf4e2,
  #          loss5_trreg_skl, loss5_tereg_skl, acc5_trreg_skl, acc5_tereg_skl, clf5e2]

  # #Save the model as a pickle in a file 
  # joblib.dump(model5 , '/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2e2.sav') 
    
  # Load the model from the file 
  model_from_joblib5 = joblib.load('/content/drive/My Drive/Colab Notebooks/MLAssignment2/model2e2.sav')  
    
  loss1_trreg_skl = model_from_joblib5[0]
  loss1_tereg_skl = model_from_joblib5[1]
  acc1_trreg_skl = model_from_joblib5[2]
  acc1_tereg_skl = model_from_joblib5[3]
  loss2_trreg_skl = model_from_joblib5[5]
  loss2_tereg_skl = model_from_joblib5[6]
  acc2_trreg_skl = model_from_joblib5[7]
  acc2_tereg_skl = model_from_joblib5[8]
  loss3_trreg_skl = model_from_joblib5[10]
  loss3_tereg_skl = model_from_joblib5[11]
  acc3_trreg_skl = model_from_joblib5[12]
  acc3_tereg_skl = model_from_joblib5[13]
  loss4_trreg_skl = model_from_joblib5[15]
  loss4_tereg_skl = model_from_joblib5[16]
  acc4_trreg_skl = model_from_joblib5[17]
  acc4_tereg_skl = model_from_joblib5[18]
  loss5_trreg_skl = model_from_joblib5[20]
  loss5_tereg_skl = model_from_joblib5[21]
  acc5_trreg_skl = model_from_joblib5[22]
  acc5_tereg_skl = model_from_joblib5[23]




  acc_table5 = pd.DataFrame(columns=['Model Number','Iterations', 'Train Accuracy','Test Accuracy', 'Train Loss', 'Test Loss'])

  for i in range(0,6):

    #add iterations column
    new_row1b = {'Model Number':'1','Iterations': iterations[i], 'Train Accuracy':acc1_trreg_skl[i] ,'Test Accuracy':acc1_tereg_skl[i], 'Train Loss':loss1_trreg_skl[i] ,'Test Loss':loss1_tereg_skl[i]}
    acc_table5 = acc_table5.append(new_row1b,ignore_index=True)
    new_row2b = {'Model Number':'2','Iterations': iterations[i],'Train Accuracy':acc2_trreg_skl[i] ,'Test Accuracy':acc2_tereg_skl[i], 'Train Loss':loss2_trreg_skl[i] ,'Test Loss':loss2_tereg_skl[i]}
    acc_table5 = acc_table5.append(new_row2b,ignore_index=True)
    new_row3b = {'Model Number':'3','Iterations': iterations[i],'Train Accuracy':acc3_trreg_skl[i] ,'Test Accuracy':acc3_tereg_skl[i], 'Train Loss':loss3_trreg_skl[i] ,'Test Loss':loss3_tereg_skl[i]}
    acc_table5 = acc_table5.append(new_row3b,ignore_index=True)
    new_row4b = {'Model Number':'4','Iterations': iterations[i],'Train Accuracy':acc4_trreg_skl[i] ,'Test Accuracy':acc4_tereg_skl[i], 'Train Loss':loss4_trreg_skl[i] ,'Test Loss':loss4_tereg_skl[i]}
    acc_table5 = acc_table5.append(new_row4b,ignore_index=True)
    new_row5b = {'Model Number':'5','Iterations': iterations[i],'Train Accuracy':acc5_trreg_skl[i] ,'Test Accuracy':acc5_tereg_skl[i], 'Train Loss':loss5_trreg_skl[i] ,'Test Loss':loss5_tereg_skl[i]}
    acc_table5 = acc_table5.append(new_row5b,ignore_index=True)

  display(acc_table5)

  fig = plt.figure()
  fig, axs = plt.subplots(nrows=5, ncols=2, figsize=(20,20))

  axs[0, 0].plot(iterations, loss1_trreg_skl, label = "Training Loss")
  axs[0, 0].plot(iterations, loss1_tereg_skl, label = "Test Loss" )
  axs[0, 0].set_xlabel("Iterations")
  axs[0, 0].set_ylabel("Loss")
  axs[0, 0].legend(loc="best")
  axs[0, 0].set_title("Model 1")
  axs[0, 1].plot(iterations, acc1_trreg_skl, label = "Training Accuracy")
  axs[0, 1].plot(iterations, acc1_tereg_skl, label = "Test Accuracy")
  axs[0, 1].set_xlabel("Iterations")
  axs[0, 1].set_ylabel("Accuracy")
  axs[0, 1].legend(loc= "best")
  axs[0, 1].set_title("Model 1")

  axs[1, 0].plot(iterations, loss2_trreg_skl, label = "Training Loss")
  axs[1, 0].plot(iterations, loss2_tereg_skl, label = "Test Loss" )
  axs[1, 0].set_xlabel("Iterations")
  axs[1, 0].set_ylabel("Loss")
  axs[1, 0].legend(loc="best")
  axs[1, 0].set_title("Model 2")
  axs[1, 1].plot(iterations, acc2_trreg_skl, label = "Training Accuracy")
  axs[1, 1].plot(iterations, acc2_tereg_skl, label = "Test Accuracy")
  axs[1, 1].set_xlabel("Iterations")
  axs[1, 1].set_ylabel("Accuracy")
  axs[1, 1].legend(loc= "best")
  axs[1, 1].set_title("Model 2")

  axs[2, 0].plot(iterations, loss3_trreg_skl, label = "Training Loss") 
  axs[2, 0].plot(iterations, loss3_tereg_skl, label = "Test Loss" )
  axs[2, 0].set_xlabel("Iterations")
  axs[2, 0].set_ylabel("Loss")
  axs[2, 0].legend(loc="best")
  axs[2, 0].set_title("Model 3")
  axs[2, 1].plot(iterations, acc3_trreg_skl, label = "Training Accuracy")
  axs[2, 1].plot(iterations, acc3_tereg_skl, label = "Test Accuracy")
  axs[2, 1].set_xlabel("Iterations")
  axs[2, 1].set_ylabel("Accuracy")
  axs[2, 1].legend(loc= "best")
  axs[2, 1].set_title("Model 3")

  axs[3, 0].plot(iterations, loss4_trreg_skl, label = "Training Loss")
  axs[3, 0].plot(iterations, loss4_tereg_skl, label = "Test Loss" )
  axs[3, 0].set_xlabel("Iterations")
  axs[3, 0].set_ylabel("Loss")
  axs[3, 0].legend(loc="best")
  axs[3, 0].set_title("Model 4")
  axs[3, 1].plot(iterations, acc4_trreg_skl, label = "Training Accuracy")
  axs[3, 1].plot(iterations, acc4_tereg_skl, label = "Test Accuracy")
  axs[3, 1].set_xlabel("Iterations")
  axs[3, 1].set_ylabel("Accuracy")
  axs[3, 1].legend(loc= "best")
  axs[3, 1].set_title("Model 4")

  axs[4, 0].plot(iterations, loss5_trreg_skl, label = "Training Loss")
  axs[4, 0].plot(iterations, loss5_tereg_skl, label = "Test Loss" )
  axs[4, 0].set_xlabel("Iterations")
  axs[4, 0].set_ylabel("Loss")
  axs[4, 0].legend(loc="best")
  axs[4, 0].set_title("Model 5")
  axs[4, 1].plot(iterations, acc5_trreg_skl, label = "Training Accuracy")
  axs[4, 1].plot(iterations, acc5_tereg_skl, label = "Test Accuracy")
  axs[4, 1].set_xlabel("Iterations")
  axs[4, 1].set_ylabel("Accuracy")
  axs[4, 1].legend(loc= "best")
  axs[4, 1].set_title("Model 5")

  fig.suptitle('Loss and Accuracy Plots for all 5 models with regularisation constant lambda = 0.1') 
  fig. tight_layout(pad=5.0)
  plt.show() 
q2a()
q2c()
q2d()
q2e()

"""**3 a**"""

def q3a():
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.io import loadmat
  import seaborn as sns
  import pandas as pd
  mats2 = loadmat('/content/drive/My Drive/Colab Notebooks/MLAssignment2/dataset_2.mat')

  data2=mats2['samples']
  labels2=mats2['labels'][0]
  unique_labels2=np.unique(labels2)
  x=np.zeros(10000)
  y=np.zeros(10000)
  for i in range(len(labels2)):
    x[i]=data2[i][0]
    y[i]=data2[i][1]
  plt.figure(figsize=(12,6))
  colors = ( 'r','y','g','b')
  scatter=plt.scatter(x,y,c=mats2["labels"],cmap=plt.cm.get_cmap("inferno",4),marker='o',s=15)
  #scatter=plt.scatter(x,y,c=mats["labels"],cmap=colors,marker='o',s=15)
  plt.xlabel("feature1")
  plt.ylabel("feature2")
  #plt.grid("False")
  plt.legend(handles=scatter.legend_elements()[0],labels=list(unique_labels2), loc= 'best')
  plt.title("scatter plot for dataset2.mat")
  plt.show()

  # sns.set(rc={'figure.figsize':(11.7,6.27)})
  # #sns.figsize(10,6)
  # sns.set_style('whitegrid')
  # sns.scatterplot(x,y,hue=labels,legend='full',palette='colorblind',s=100)

q2a()
q2c()
q2d()
q2e()
q3a()

"""**3 c**"""

def q3c():
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.io import loadmat
  import seaborn as sns
  import pandas as pd
  from random import seed,randrange
  mats2 = loadmat('/content/drive/My Drive/Colab Notebooks/MLAssignment2/dataset_2.mat')

  data2=mats2['samples']
  labels2=mats2['labels'][0]
  unique_labels2=np.unique(labels2)

  # Split a dataset into k folds
  def kfold_split(dataset,labels,folds):
    dataset_split=list()
    labels_split=list()
    dataset_copy=dataset.tolist()
    labels_copy=labels.tolist()
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
      fold=list()
      foldlabel=list()
      while len(fold)<fold_size:
        index = randrange(len(dataset_copy))
        #p=np.random.permutation(len(dataset_copy))
        fold.append(dataset_copy.pop(index))
        foldlabel.append(labels_copy.pop(index))
      dataset_split.append(fold)
      labels_split.append(foldlabel)
    return dataset_split,labels_split

  seed(1)
  folds2, labelfolds2 = kfold_split(data2, labels2, 5)
  
  
  (train_samples1c) = np.array(folds2[0]+folds2[1]+folds2[2]+folds2[3])
  train_labels1c = np.array(labelfolds2[0]+labelfolds2[1]+labelfolds2[2]+labelfolds2[3])
  test_sample1c = np.array(folds2[4])
  test_label1c = np.array(labelfolds2[4])

  train_samples2c = np.array(folds2[0]+folds2[1]+folds2[2]+folds2[4])
  train_labels2c = np.array(labelfolds2[0]+labelfolds2[1]+labelfolds2[2]+labelfolds2[4])
  test_sample2c = np.array(folds2[3])
  test_label2c = np.array(labelfolds2[3])

  train_samples3c = np.array(folds2[0]+folds2[1]+folds2[3]+folds2[4])
  train_labels3c = np.array(labelfolds2[0]+labelfolds2[1]+labelfolds2[3]+labelfolds2[4])
  test_sample3c = np.array(folds2[2])
  test_label3c = np.array(labelfolds2[2])

  train_samples4c = np.array(folds2[0]+folds2[2]+folds2[3]+folds2[4])
  train_labels4c = np.array(labelfolds2[0]+labelfolds2[2]+labelfolds2[3]+labelfolds2[4])
  test_sample4c = np.array(folds2[1])
  test_label4c = np.array(labelfolds2[1])

  train_samples5c = np.array(folds2[1]+folds2[2]+folds2[3]+folds2[4])
  train_labels5c = np.array(labelfolds2[1]+labelfolds2[2]+labelfolds2[3]+labelfolds2[4])
  test_sample5c = np.array(folds2[0])
  test_label5c = np.array(labelfolds2[0])

  def onevsrestlogreg(train_samples, train_labels, test_sample, test_label):
    class_of_interest = [0, 1, 2, 3]
    loss_trainlst = []
    loss_testlst = []
    acc_trainlst = []
    acc_testlst = []
    predlbltest = []
    predlbltrain =  []

    for i in class_of_interest:
      logreg_obj = LogRegression(train_samples)
      # y_binary = np.zeros((y.shape))
      # y_binary = np.where(y == class_of_interest, 1, y_binary)
      theta, intercept, loss_train, acc_train = logreg_obj.fit(train_samples, train_labels, 1, 0.1, i, 1)
      theta_test, intercept_test, loss_test, acc_test = logreg_obj.fit(test_sample, test_label, 1, 0.1, i, 1)
      labels_predict_test = logreg_obj.probab_predict(test_sample)
      labels_predict_train = logreg_obj.probab_predict(train_samples)
      loss_trainlst.append(loss_train)
      loss_testlst.append(loss_test)
      acc_trainlst.append(acc_train)
      acc_testlst.append(acc_test)
      predlbltest.append(labels_predict_test)
      predlbltrain.append(labels_predict_train)
    # an_array1 = np.zeros(test_label.shape)  
    # an_array2 = np.zeros(test_label.shape)
    # an_array3 = np.zeros(test_label.shape)
    # an_array4 = np.zeros(test_label.shape)
    # an_array1 = np.where(predlbltest[0] == 1, 0, an_array1)
    # an_array2 = np.where(predlbltest[1] == 1, 1, an_array2)
    # an_array3 = np.where(predlbltest[2] == 1, 2, an_array3)
    # an_array4 = np.where(predlbltest[3] == 1, 3, an_array4)
    max_array_test = np.zeros(test_label.shape)
    max_array_train = np.zeros(train_labels.shape)
    for i in range(test_label.shape[0]):
      #max_array[i] = max(an_array1[i], an_array2[i], an_array3[i], an_array4[i])
      if (predlbltest[0][i] > predlbltest[1][i] and  predlbltest[0][i] > predlbltest[2][i] and predlbltest[0][i] >predlbltest[3][i]):
          max_array_test[i] = 0
      elif (predlbltest[1][i] > predlbltest[0][i] and  predlbltest[1][i] > predlbltest[2][i] and predlbltest[1][i] >predlbltest[3][i]):
          max_array_test[i] = 1
      elif (predlbltest[2][i] > predlbltest[0][i] and  predlbltest[2][i] > predlbltest[1][i] and predlbltest[2][i] >predlbltest[3][i]):
          max_array_test[i] = 2
      elif (predlbltest[3][i] > predlbltest[0][i] and  predlbltest[3][i] > predlbltest[1][i] and predlbltest[3][i] >predlbltest[2][i]):
          max_array_test[i] = 3
    for i in range(train_labels.shape[0]):
      if (predlbltrain[0][i] > predlbltrain[1][i] and  predlbltrain[0][i] > predlbltrain[2][i] and predlbltrain[0][i] >predlbltrain[3][i]):
          max_array_train[i] = 0
      elif (predlbltrain[1][i] > predlbltrain[0][i] and  predlbltrain[1][i] > predlbltrain[2][i] and predlbltrain[1][i] >predlbltrain[3][i]):
          max_array_train[i] = 1
      elif (predlbltrain[2][i] > predlbltrain[0][i] and  predlbltrain[2][i] > predlbltrain[1][i] and predlbltrain[2][i] >predlbltrain[3][i]):
          max_array_train[i] = 2
      elif (predlbltrain[3][i] > predlbltrain[0][i] and  predlbltrain[3][i] > predlbltrain[1][i] and predlbltrain[3][i] >predlbltrain[2][i]):
          max_array_train[i] = 3

      #max_array_test[i] = max(predlbltest[0][i], predlbltest[1][i], predlbltest[2][i], predlbltest[3][i])
      #max_array_train[i] = max(predlbltrain[0][i], predlbltrain[1][i], predlbltrain[2][i], predlbltrain[3][i])
    overall_accuracy = np.sum(test_label == max_array_test)/test_sample.shape[0]
    accu_train = np.sum(train_labels == max_array_train)/train_samples.shape[0]
    #print(" model : ", "Accuracy ", accuracy_iter)
    
    return loss_trainlst, loss_testlst, acc_trainlst, acc_testlst, predlbltest, predlbltrain, overall_accuracy, accu_train 
  # commented to check the saved model
  # loss1_train3c, loss1_test3c, acc1_train3c, acc1_test3c, pred1_test3c, pred1_train3c, overall_accuracy1_3c, accu_train1_3c = onevsrestlogreg(train_samples1c, train_labels1c, test_sample1c, test_label1c)
  # loss2_train3c, loss2_test3c, acc2_train3c, acc2_test3c, pred2_test3c, pred2_train3c, overall_accuracy2_3c, accu_train2_3c = onevsrestlogreg(train_samples2c, train_labels2c, test_sample2c, test_label2c)
  # loss3_train3c, loss3_test3c, acc3_train3c, acc3_test3c, pred3_test3c, pred3_train3c, overall_accuracy3_3c, accu_train3_3c = onevsrestlogreg(train_samples3c, train_labels3c, test_sample3c, test_label3c)
  # loss4_train3c, loss4_test3c, acc4_train3c, acc4_test3c, pred4_test3c, pred4_train3c, overall_accuracy4_3c, accu_train4_3c = onevsrestlogreg(train_samples4c, train_labels4c, test_sample4c, test_label4c)
  # loss5_train3c, loss5_test3c, acc5_train3c, acc5_test3c, pred5_test3c, pred5_train3c, overall_accuracy5_3c, accu_train5_3c = onevsrestlogreg(train_samples5c, train_labels5c, test_sample5c, test_label5c)


  # model6 = [loss1_train3c, loss1_test3c, acc1_train3c, acc1_test3c, pred1_test3c, pred1_train3c,overall_accuracy1_3c, accu_train1_3c,
  #          loss2_train3c, loss2_test3c, acc2_train3c, acc2_test3c, pred2_test3c, pred2_train3c, overall_accuracy2_3c, accu_train2_3c,
  #          loss3_train3c, loss3_test3c, acc3_train3c, acc3_test3c, pred3_test3c, pred3_train3c, overall_accuracy3_3c, accu_train3_3c,
  #          loss4_train3c, loss4_test3c, acc4_train3c, acc4_test3c, pred4_test3c, pred4_train3c, overall_accuracy4_3c, accu_train4_3c,
  #          loss5_train3c, loss5_test3c, acc5_train3c, acc5_test3c, pred5_test3c, pred5_train3c, overall_accuracy5_3c, accu_train5_3c]

  # #Save the model as a pickle in a file 
  # joblib.dump(model6 , '/content/drive/My Drive/Colab Notebooks/MLAssignment2/model3c.sav') 
    
  # Load the model from the file 
  model_from_joblib6 = joblib.load('/content/drive/My Drive/Colab Notebooks/MLAssignment2/model3c.sav')  
    
  loss1_train3c = model_from_joblib6[0]
  loss1_test3c = model_from_joblib6[1]
  acc1_train3c = model_from_joblib6[2]
  acc1_test3c = model_from_joblib6[3]
  loss2_train3c = model_from_joblib6[8]
  loss2_test3c = model_from_joblib6[9]
  acc2_train3c = model_from_joblib6[10]
  acc2_test3c = model_from_joblib6[11]
  loss3_train3c = model_from_joblib6[16]
  loss3_test3c = model_from_joblib6[17]
  acc3_train3c = model_from_joblib6[18]
  acc3_test3c = model_from_joblib6[19]
  loss4_train3c = model_from_joblib6[24]
  loss4_test3c = model_from_joblib6[25]
  acc4_train3c = model_from_joblib6[26]
  acc4_test3c = model_from_joblib6[27]
  loss5_train3c = model_from_joblib6[32]
  loss5_test3c = model_from_joblib6[33]
  acc5_train3c = model_from_joblib6[34]
  acc5_test3c = model_from_joblib6[35]

  print("overall test accuracy : \n")
  print("Fold 1 : ", model_from_joblib6[6])
  print("Fold 2 : ", model_from_joblib6[14])
  print("Fold 3 : ", model_from_joblib6[22])
  print("Fold 4 : ", model_from_joblib6[30])
  print("Fold 5 : ", model_from_joblib6[38])

  print("overall train accuracy : \n")
  print("Fold 1 : ", model_from_joblib6[7])
  print("Fold 2 : ", model_from_joblib6[15])
  print("Fold 3 : ", model_from_joblib6[23])
  print("Fold 4 : ", model_from_joblib6[31])
  print("Fold 5 : ", model_from_joblib6[39])

  acc_table6 = pd.DataFrame(columns=['Model Number','Class', 'Train Accuracy','Test Accuracy', 'Train Loss', 'Test Loss'])

  for i in range(4):
    #add iterations column
    new_row1_3c = {'Model Number':'1','Class': i, 'Train Accuracy':acc1_train3c[i][5] ,'Test Accuracy':acc1_test3c[i][5], 'Train Loss':loss1_train3c[i][5] ,
                  'Test Loss':loss1_test3c[i][5]}
    acc_table6 = acc_table6.append(new_row1_3c,ignore_index=True)
    new_row2_3c = {'Model Number':'2','Class': i,'Train Accuracy':acc2_train3c[i][5] ,'Test Accuracy':acc2_test3c[i][5], 'Train Loss':loss2_train3c[i][5] ,
                  'Test Loss':loss2_test3c[i][5]}
    acc_table6 = acc_table6.append(new_row2_3c,ignore_index=True)
    new_row3_3c = {'Model Number':'3','Class': i,'Train Accuracy':acc3_train3c[i][5] ,'Test Accuracy':acc3_test3c[i][5], 'Train Loss':loss3_train3c[i][5] ,
                  'Test Loss':loss3_test3c[i][5]}
    acc_table6 = acc_table6.append(new_row3_3c,ignore_index=True)
    new_row4_3c = {'Model Number':'4','Class': i,'Train Accuracy':acc4_train3c[i][5] ,'Test Accuracy':acc4_test3c[i][5], 'Train Loss':loss4_train3c[i][5] ,
                  'Test Loss':loss4_test3c[i][5]}
    acc_table6 = acc_table6.append(new_row4_3c,ignore_index=True)
    new_row5_3c = {'Model Number':'5','Class': i,'Train Accuracy':acc5_train3c[i][5] ,'Test Accuracy':acc5_test3c[i][5], 'Train Loss':loss5_train3c[i][5] ,
                  'Test Loss':loss5_test3c[i][5]}
    acc_table6 = acc_table6.append(new_row5_3c,ignore_index=True)

  display(acc_table6)


q2a()
q2c()
q2d()
q2e()
q3a()
q3c()

# print(loss1_train3c,"\n", loss1_test3c,"\n", acc1_train3c,"\n", acc1_test3c)
# for i in range(4):
#     print( "          Train loss      Test Loss      Train Accuracy     Test Accuracy")
#     print( "class :", i)
#     print("fold1 : ", loss1_train3c[i][5], loss1_test3c[i][5], acc1_train3c[i][5], acc1_test3c[i][5], "\n")
#     print("fold2 : ", loss2_train3c[i][5], loss2_test3c[i][5], acc2_train3c[i][5], acc2_test3c[i][5], "\n")
#     print("fold3 : ", loss3_train3c[i][5], loss3_test3c[i][5], acc3_train3c[i][5], acc3_test3c[i][5], "\n")
#     print("fold4 : ", loss4_train3c[i][5], loss4_test3c[i][5],acc4_train3c[i][5], acc4_test3c[i][5], "\n")
#     print("fold5 : ", loss5_train3c[i][5], loss5_test3c[i][5],acc5_train3c[i][5], acc5_test3c[i][5], "\n")

"""**Answer 3b**"""

def q3b():
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.io import loadmat
  import seaborn as sns
  import pandas as pd

  mats2 = loadmat('/content/drive/My Drive/Colab Notebooks/MLAssignment2/dataset_2.mat')
  data2 = mats2['samples']
  labels2 = mats2['labels'][0]
  unique_labels2 = np.unique(labels2)

  X = data2.copy()
  y = labels2.copy()
  y = y[:,np.newaxis]
  X_test_data = []
  X_test_labels = []
  #kfold
  # Split a dataset into k folds
  def kfold_split(dataset,labels,folds):
    dataset_split=list()
    labels_split=list()
    dataset_copy=dataset.tolist()
    labels_copy=labels.tolist()
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
      fold=list()
      foldlabel=list()
      while len(fold)<fold_size:
        index = randrange(len(dataset_copy))
        fold.append(dataset_copy.pop(index))
        foldlabel.append(labels_copy.pop(index))
      dataset_split.append(fold)
      labels_split.append(foldlabel)
    return dataset_split,labels_split

  seed(1)
  folds3b, labelfolds3b = kfold_split(X, y, 5)

  for i in range(len(folds3b)):
    X_test_data = (np.array(folds3b[i]))
    X_test_labels =  (np.array(labelfolds3b[i]))
    X_train_data = []
    X_train_labels = []
    for j in range(len(folds3b)):
      if i !=j:
        train_data = folds3b[j]
        train_labels = labelfolds3b[j]
        X_train_data = X_train_data + (train_data)
        X_train_labels = X_train_labels + (train_labels)   
    final_data = np.concatenate((np.array(X_train_data),np.array(X_train_labels)),axis=1)
    column_values = ['data0', 'data1', 'labels']
    # creating the dataframe 
    df = pd.DataFrame(data = final_data,  
                      columns = column_values)
    classdflst = []
    grouped = df.groupby(df.labels)
    classdflst = [grouped.get_group(0), grouped.get_group(1), grouped.get_group(2), grouped.get_group(3)]
    #preprocessing
    ovo = []
    for k in range(4):
      for l in range(k+1 , 4):
        ovo.append(pd.concat([classdflst[k], classdflst[l]]))
    ovo[1]["labels"].replace(2, 1, inplace = True) # (0,2) = (0,1)
    ovo[2]["labels"].replace(3, 1, inplace = True) # (0,3) = (0,1)
    ovo[3]["labels"].replace(2, 0, inplace = True) # (1,2) = (1,0)
    ovo[4]["labels"].replace(3, 0, inplace = True) # (1,3) = (1,0)
    ovo[5]["labels"].replace(2, 0, inplace = True) # (2,3)
    ovo[5]["labels"].replace(3, 1, inplace = True) # (2,3) = (0,1)
    final_ovo_data = []
    final_ovo_labels = []
    labels_lst = []
    for m in range(6):
      final_ovo_data.append(pd.concat((ovo[m]["data0"], ovo[m]["data1"]), axis=1))
      final_ovo_labels.append(ovo[m]["labels"])

      logreg3b = LogRegression(final_ovo_data[m].to_numpy())
      theta, intercept, loss_train, acc_train = logreg3b.fit(final_ovo_data[m].to_numpy(), final_ovo_labels[m].to_numpy(), 1, 0.1, 0, 0)
      theta_test, intercept_test, loss_test, acc_test = logreg3b.fit(X_test_data, X_test_labels.flatten(), 1, 0.1, 0, 0)
      labels_predict_test = logreg3b.predict((X_test_data))
      #labels_predict_train = logreg3b.predict(final_ovo_data[m].to_numpy())
      for z in range(len(labels_predict_test)):
        if m == 1:
          if labels_predict_test[z] == 1:
            labels_predict_test[z] = 2
          else: 
            labels_predict_test[z] = 0

          #[2 if x==1 else 0 for x in labels_predict_test]
        elif m == 2:
          if labels_predict_test[z] == 1:
            labels_predict_test[z] = 3
          else: 
            labels_predict_test[z] = 0
          #[3 if x==1 else 0 for x in labels_predict_test]
        elif m == 3:
          if labels_predict_test[z] == 0:
            labels_predict_test[z] = 2
          else: 
            labels_predict_test[z] = 1
          #[2 if x==0 else 1 for x in labels_predict_test]
        elif m == 4:
          if labels_predict_test[z] == 0:
            labels_predict_test[z] = 3
          else: 
            labels_predict_test[z] = 1
          #[3 if x==0 else 1 for x in labels_predict_test]
        elif m == 5:
          if labels_predict_test[z] == 0:
            labels_predict_test[z] = 2
          else: 
            labels_predict_test[z] = 3
          #[3 if x==1 else 2 for x in labels_predict_test]
      labels_lst.append(labels_predict_test)
    #print(labels_lst)
    #voting
    from statistics import mode 
    from collections import Counter 
    #for majority vote-test data
    column=np.zeros(len(labels_predict_test))
    #column_train=np.zeros(len())
    major=np.zeros(len(labels_predict_test))
    #major_train=np.zeros(len(labels_train3c))
    for n in range(len(labels_predict_test)):
      for p in range(6):
        column[p]=labels_lst[p][n]
        #y_final.append(labels_lst[p][n])
      # temp=Counter(column)
      # major[n],x =temp.most_common(1)[0]
      major[n] = mode(column)
    accuracy_test_final  = np.sum(major == X_test_labels.flatten())/X_test_data.shape[0]
      #y_mode = mode(y_final)   
    #print(major)
    print("fold : ", i+1, ": ", accuracy_test_final)
    # Classwise
    count0 = 0
    count1 = 0
    count2 = 0
    count3 = 0
    count_actual0=0
    count_actual1 = 0
    count_actual2=0
    count_actual3=0
    #print("Classwise accuracy for OVR ", i+1)
    for k in range(len(X_test_labels)):
      if(major[k]== 0 and X_test_labels[k] == 0):
        count0 = count0 + 1
      elif(major[k]== 1 and X_test_labels[k] == 1):
        count1 = count1 + 1
      elif(major[k]== 2 and X_test_labels[k] == 2):
        count2= count2 + 1
      elif(major[k]== 3 and X_test_labels[k] == 3):
        count3 = count3 + 1
      if(X_test_labels[k] == 0):
        count_actual0 = count_actual0 + 1
      elif(X_test_labels[k] == 1):
        count_actual1 = count_actual1 + 1
      elif(X_test_labels[k] == 2):
        count_actual2 = count_actual2 + 1
      elif(X_test_labels[k] == 3):
        count_actual3 = count_actual3 + 1
    # print(" class 0 : ", count0/count_actual0, "\n")
    # print(" class 1 : ", count1/count_actual1, "\n")
    # print(" class 2 : ", count2/count_actual2, "\n")
    # print(" class 3 : ", count3/count_actual3, "\n")

# q2a()
# q2c()
# q2d()
# q2e()
# q3a()
# q3c()
q3b()

"""**Answer 3d**"""

def q3d():
  from sklearn.linear_model import LogisticRegression
  from sklearn.preprocessing import StandardScaler
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.io import loadmat
  import seaborn as sns
  import pandas as pd
  #from sklearn.model_selection import train_test_split
  from sklearn.multiclass import OneVsOneClassifier
  from sklearn.svm import LinearSVC

  mats2 = loadmat('/content/drive/My Drive/Colab Notebooks/MLAssignment2/dataset_2.mat')
  data2 = mats2['samples']
  labels2 = mats2['labels'][0]
  unique_labels2 = np.unique(labels2)
  X = data2.copy()
  y = labels2.copy()
  #y = y[:,np.newaxis]
  X_test_data = []
  X_test_labels = []
  # Split a dataset into k folds
  def kfold_split(dataset,labels,folds):
    dataset_split=list()
    labels_split=list()
    dataset_copy=dataset.tolist()
    labels_copy=labels.tolist()
    fold_size = int(len(dataset) / folds)
    for i in range(folds):
      fold=list()
      foldlabel=list()
      while len(fold)<fold_size:
        index = randrange(len(dataset_copy))
        #p=np.random.permutation(len(dataset_copy))
        fold.append(dataset_copy.pop(index))
        foldlabel.append(labels_copy.pop(index))
      dataset_split.append(fold)
      labels_split.append(foldlabel)
    return dataset_split,labels_split

  seed(1)
  #kfold
  folds3d, labelfolds3d = kfold_split(X, y, 5)

  for i in range(len(folds3d)):
    X_test_data = (np.array(folds3d[i]))
    X_test_labels =  (np.array(labelfolds3d[i]))
    X_train_data = []
    X_train_labels = []
    for j in range(len(folds3d)):
      if i !=j:
        train_data = folds3d[j]
        train_labels = labelfolds3d[j]
        X_train_data = X_train_data + (train_data) 
        X_train_labels = X_train_labels + (train_labels)

    # Create one-vs-rest logistic regression object
    clf = LogisticRegression(random_state=0, multi_class='ovr', C = 10, max_iter = 1000)
    # Train model
    model = clf.fit(X=X_train_data, y=X_train_labels)
    # Predict class
    y_pred = model.predict(X_test_data)
    # View predicted probabilities
    yprobab = model.predict_proba(X_test_data)
    accuracy = np.sum(y_pred == X_test_labels)/X_test_data.shape[0]
    print("OVR FOLD Accuracy", i+1, ": " , accuracy)
    # Classwise
    count0 = 0
    count1 = 0
    count2 = 0
    count3 = 0
    count_actual0=0
    count_actual1 = 0
    count_actual2=0
    count_actual3=0
    print("Classwise accuracy for OVR ", i+1)
    for k in range(len(X_test_labels)):
      if(y_pred[k]== 0 and X_test_labels[k] == 0):
        count0 = count0 + 1
      elif(y_pred[k]== 1 and X_test_labels[k] == 1):
        count1 = count1 + 1
      elif(y_pred[k]== 2 and X_test_labels[k] == 2):
        count2= count2 + 1
      elif(y_pred[k]== 3 and X_test_labels[k] == 3):
        count3 = count3 + 1
      if(X_test_labels[k] == 0):
        count_actual0 = count_actual0 + 1
      elif(X_test_labels[k] == 1):
        count_actual1 = count_actual1 + 1
      elif(X_test_labels[k] == 2):
        count_actual2 = count_actual2 + 1
      elif(X_test_labels[k] == 3):
        count_actual3 = count_actual3 + 1
    print(" class 0 : ", count0/count_actual0, "\n")
    print(" class 1 : ", count1/count_actual1, "\n")
    print(" class 2 : ", count2/count_actual2, "\n")
    print(" class 3 : ", count3/count_actual3, "\n")

    clf2 = OneVsOneClassifier(LogisticRegression(max_iter = 1000, random_state=0))
    clf2.fit(X_train_data, X_train_labels)
    ypred2 = clf2.predict(X_test_data)
    ypred2_train = clf2.predict(X_train_data)
    accuracy2_test = np.sum(ypred2 == X_test_labels)/X_test_data.shape[0]
    print("OVO FOLD Accuracy", i+1, ": " , accuracy2_test)
    # Classwise
    count0r = 0
    count1r = 0
    count2r = 0
    count3r = 0
    count_actual0r=0
    count_actual1r = 0
    count_actual2r=0
    count_actual3r=0
    print("Classwise accuracy for OVO ", i+1)
    for m in range(len(X_test_labels)):
      if(ypred2[m]== 0 and X_test_labels[m] == 0):
        count0r = count0r + 1
      if(ypred2[m]== 1 and X_test_labels[m] == 1):
        count1r = count1r + 1
      if(ypred2[m]== 2 and X_test_labels[m] == 2):
        count2r= count2r + 1
      if(ypred2[m]== 3 and X_test_labels[m] == 3):
        count3r = count3r + 1
      if(X_test_labels[m] == 0):
        count_actual0r = count_actual0r + 1
      if(X_test_labels[m] == 1):
        count_actual1r = count_actual1r + 1
      if(X_test_labels[m] == 2):
        count_actual2r = count_actual2r + 1
      if(X_test_labels[m] == 3):
        count_actual3r = count_actual3r + 1
    print(" class 0 : ", count0r/count_actual0r, "\n")
    print(" class 1 : ", count1r/count_actual1r, "\n")
    print(" class 2 : ", count2r/count_actual2r, "\n")
    print(" class 3 : ", count3r/count_actual3r, "\n")

q3d()

if __name__ == "__main__":
  q2a()
  q2c()
  q2d()
  q2e()
  q3a()
  q3c()
  q3b()
  q3d()